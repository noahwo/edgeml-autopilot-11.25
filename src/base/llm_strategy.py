"""!
LLMStrategy Module

[Documents generated by Claude 3.5 Sonnet, human revised.]

This module implements the Strategy pattern for different Language Model (LLM) providers,
offering a unified interface for model interactions. It supports multiple LLM backends
including OpenAI, Ollama, and LiteLLM.

Key Components:
- LLMStrategy: Abstract base class defining the interface
- OpenAILiteLLMStrategy: OpenAI implementation using LiteLLM
- OllamaLiteLLMStrategy: Ollama implementation
- OpenAIStrategy: Direct OpenAI API implementation
- LiteLLMStrategy: Generic LiteLLM implementation

Features:
- Standardized interface for model invocation
- Support for metadata and logging
- Configurable API endpoints
- Error handling and logging
- Integration with LangFuse for monitoring

Usage Example:
    strategy = OpenAILiteLLMStrategy(api_key="your-key", model_name="gpt-4")
    response = strategy.invoke("Your prompt", metadata={"user_id": "123"})
"""

import logging
import os
import re
import socket
from abc import ABC, abstractmethod
from langchain_core.prompts import PromptTemplate

import litellm
from langsmith.wrappers import wrap_openai
from litellm import completion  # Assumed import from LiteLLM library
from openai import OpenAI  # Importing OpenAI for the new strategy



class LLMStrategy(ABC):
    """!
    Abstract base class defining the interface for language model strategies.

    This class enforces a consistent interface across different LLM implementations,
    making them interchangeable in the application.
    """

    def __init__(self, model_name: str, parameters: dict|bool=False):
        self.parameters = parameters
        self.model_name = model_name
 

    @abstractmethod
    def get_endpoint_url(self) -> str:
        """!
        Retrieves the API endpoint URL for the language model.

        Returns:
            The API endpoint URL as a string.
        """
        pass
    
    def use_parameters(self):
        """!
        Use parameters for the language model.
        """
        parameters = self.parameters
        if isinstance(parameters, dict):
            return parameters
        elif isinstance(parameters, bool):
            if parameters:
                return {"temperature": 0.1,
                "top_p": 0.3,}
            else:
                return {}
        else:
            raise ValueError("Invalid parameters")
            
    def invoke(
        self,
        prompts: list[str | PromptTemplate] | str,
        metadata_: dict,
        model_provider: str,
        lib_extract_model: str | None = None,
    ):
        """!
        Invokes the Ollama model with a single prompt.

        Args:
            prompt (str): The input text to send to the model
            metadata_ (dict): Metadata for request tracking

        Returns:
            str: The generated response from the language model
        """
        litellm.success_callback = ["langfuse"]
        if lib_extract_model:
            self.model_name = lib_extract_model

        params = self._get_completion_parameters(metadata_, prompts, model_provider)
        response = completion(**params)
        response_content = response.choices[0].message.content

        if (
            response_content.lstrip().startswith("<think>")
            or "<think>" in response_content
        ):
            # Remove everything between and including <think> and </think>
            cleaned_text = re.sub(
                r"<think>.*?</think>", "", response_content, flags=re.DOTALL
            )
            # Remove any extra whitespace that might be left
            cleaned_text = cleaned_text.strip()
            return cleaned_text

        return response_content

    def _get_completion_parameters(
        self, metadata_: dict, prompts: list[str | PromptTemplate] | str, model_provider: str
    ) -> dict:
        """!
        Constructs the parameters for the completion function.

        Args:
            metadata_ (dict): Metadata for request tracking and monitoring
            prompts (list[str] | str): List of role-specific prompts [system_1, system_2, user]
            model_provider (str): Model provider

        Returns:
            dict: Parameters dictionary for the completion function including:
                - model: Formatted model name (ollama/{model_name})
                - messages: Formatted message list with roles
                - metadata: Request metadata
                - api_base: Optional API endpoint URL
        """

        if isinstance(prompts, list):
            messages = [
                {"role": "system", "content": prompts[0] + "\n" + prompts[1]},
                {"role": "user", "content": prompts[2]},
            ]
        else:
            messages = [{"role": "user", "content": prompts}]
            
        if model_provider == "ollama":
            # api_base = (
            #     self.api_base
            #     if str(socket.gethostname()) == "svm-125.cs.helsinki.fi"
            #     else "http://localhost:11434"
            # )
            
            model_params={"model": f"ollama/{self.model_name}","api_base": self.api_base
                if str(socket.gethostname()) == "svm-125.cs.helsinki.fi"
                else "http://localhost:11434"}
        elif model_provider == "openai":
            model_params={"model": self.model_name}
        else:
            raise ValueError(f"Invalid model provider: {model_provider}")
        
        return {
            **model_params,
            "messages": messages,
            "metadata": metadata_,
            **self.use_parameters(),
        }

        # if model_provider == "ollama":
        #     api_base = (
        #         self.api_base
        #         if str(socket.gethostname()) == "svm-125.cs.helsinki.fi"
        #         else "http://localhost:11434"
        #     )
        #     return {
        #         "model": f"ollama/{self.model_name}",
        #         "api_base": api_base,
        #         "messages": messages,
        #         "metadata": metadata_,
        #         **self.use_parameters(),
        #     }

        # if model_provider == "openai":
        #     return {
        #         "model": self.model_name,
        #         "messages": messages,
        #         "metadata": metadata_,
        #         **self.use_parameters(),
        #     }
        # else:
        #     raise ValueError(f"Invalid model provider: {model_provider}")


class OpenAILiteLLMStrategy(LLMStrategy):
    """!
    OpenAI implementation through LiteLLM.

    This strategy provides OpenAI model access with additional features like:
    - LangFuse integration for monitoring
    - Metadata support for tracking
    - Structured response handling

    Attributes:
        api_key (str): OpenAI API authentication key
        model_name (str): Name of the OpenAI model (e.g., "gpt-4", "gpt-3.5-turbo")
        logger (Logger): Class-specific logger instance
    """

    def __init__(self, model_name, parameters: dict|bool=False):
        """!
        Initializes the LangchainOpenAIStrategy with API key and model name.

        Args:
            api_key: OpenAI API key.
            model_name: Name of the OpenAI model to use.
        """
        super().__init__(model_name, parameters)
        # self.api_key = os.getenv("OPENAI_API_KEY")
        self.logger = logging.getLogger(self.__class__.__name__)

    def invoke(self, prompts: list[str | PromptTemplate] | str, metadata_: dict, model_provider: str = "openai", lib_extract_model: str | None = None):
        return super().invoke(
            prompts,
            metadata_,
            model_provider=model_provider,
            lib_extract_model=lib_extract_model,
        )

    def deprecated_invoke(
        self, prompts: list[str | PromptTemplate] | str, metadata_, lib_extract_model=None
    ):
        """!
        Invokes the OpenAI language model with the given prompt.

        Args:
            prompt: The prompt to send to the language model.

        Returns:
            The response from the language model.
        """
        litellm.success_callback = ["langfuse"]
        if lib_extract_model:
            self.model_name = lib_extract_model

        params = self._get_completion_parameters(
            metadata_, prompts, model_provider="openai"
        )
        response = completion(**params)
        response_content = response.choices[0].message.content
        cleaned_content = self.clean_response(response_content)
        return cleaned_content

    def get_endpoint_url(self) -> str:
        """!
        Retrieves the OpenAI API endpoint URL.

        Returns:
            The OpenAI API endpoint URL as a string.
        """
        return "https://api.openai.com/v1/chat/completions"


class OllamaLiteLLMStrategy(LLMStrategy):
    """!
    Ollama implementation using LiteLLM as the backend.

    This strategy provides local model access through Ollama with features like:
    - Dynamic host configuration based on environment
    - Support for system and user roles in prompts
    - Flexible API base URL configuration

    Attributes:
        model_name (str): Name of the Ollama model
        api_base (str): Base URL for the Ollama API
    """

    def __init__(self, model_name, parameters: dict|bool=False):
        super().__init__(model_name, parameters)
        try:
            from main import OLLAMA_API_BASE
            self.api_base = OLLAMA_API_BASE
        except:
            self.api_base = os.getenv("OLLAMA_BASE_URL")
        self.logger = logging.getLogger(self.__class__.__name__)

    def invoke(self, prompts: list[str | PromptTemplate] | str, metadata_: dict,  model_provider: str = "ollama",lib_extract_model:str|None=None):
        return super().invoke(
            prompts,
            metadata_,
            model_provider=model_provider,
            lib_extract_model=lib_extract_model,
        )

    def get_endpoint_url(self):
        return int(-2)


class OpenAIStrategy(LLMStrategy):
    """!
    Concrete implementation of LLMStrategy for OpenAI's language models using the OpenAI API directly.

    Features:
    - Direct OpenAI API integration without intermediary layers
    - LangSmith integration for monitoring
    - Structured response handling

    Attributes:
        api_key (str): OpenAI API authentication key
        model_name (str): Name of the OpenAI model
        client (OpenAI): Wrapped OpenAI client instance with LangSmith integration
    """

    def __init__(self, api_key, model_name, parameters: dict|bool=False):
        """!
        Initializes the DirectOpenAIStrategy with API key and model name.

        Args:
            api_key: OpenAI API key.
            model_name: Name of the OpenAI model to use.
        """
        super().__init__(model_name, parameters)
        self.api_key = api_key
        self.client = wrap_openai(OpenAI(api_key=self.api_key))

    def invoke(self, prompt):
        """!
        Invokes the OpenAI language model with the given prompt using the direct API.

        Args:
            prompt (str): The input text to send to the model

        Returns:
            str: The generated response text from the model

        Note:
            Uses model_dump() to extract content from the structured response
        """
        response = self.client.chat.completions.create(
            model=self.model_name, messages=[{"role": "user", "content": prompt}]
        )
        # return response.choices[0].message["content"]
        return response.model_dump()["choices"][0]["message"]["content"]

    def get_endpoint_url(self) -> str:
        """
        Retrieves the OpenAI API endpoint URL.

        Returns:
            The OpenAI API endpoint URL as a string.
        """
        return "https://api.openai.com/v1/chat/completions"
